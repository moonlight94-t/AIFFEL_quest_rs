{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {},
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting trl\n",
      "  Downloading trl-0.18.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
      "Collecting datasets>=3.0.0 (from trl)\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers>=4.50.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.52.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.31.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.15)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.50.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.50.0->trl) (0.21.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.11.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Downloading trl-0.18.1-py3-none-any.whl (366 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, datasets, trl\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.4\n",
      "    Uninstalling datasets-2.14.4:\n",
      "      Successfully uninstalled datasets-2.14.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 trl-0.18.1\n"
     ]
    }
   ],
   "source": [
    "W:t!pip install trl accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "import datasets\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead"
   ],
   "metadata": {},
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/ko-gpt-trinity-1.2B-v0.5\")"
   ],
   "metadata": {},
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86cd5718b2264f7bb5fec8ec5535ef66"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/731 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9539caf696134ec59f70e9ce502c88ac"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4c1f99e956742c5a892fa24fa7f05ba"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f77c3da77457480daf09b8df8389f835"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "ppo_config = {\n",
    "    \"output_dir\": \"/content/drive/MyDrive/PPO\",\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "    \"per_device_eval_batch_size\" : 1,\n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"logging_steps\": 100,\n",
    "    \"save_steps\": 100,\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"eval_strategy\": \"no\",\n",
    "    \"report_to\": \"tensorboard\",\n",
    "    \"bf16\": True,\n",
    "    \"remove_unused_columns\": True,\n",
    "    \"logging_dir\": \"/content/drive/MyDrive/PPO/logs\",\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # PPO-specific\n",
    "    \"num_ppo_epochs\": 4,\n",
    "    \"cliprange\": 0.2,\n",
    "    \"cliprange_value\": 0.2,\n",
    "    \"vf_coef\": 0.1,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lam\": 0.95,\n",
    "    \"kl_coef\": 0.05,\n",
    "}"
   ],
   "metadata": {},
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"no_repeat_ngram_size\": 4,\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 40,\n",
    "    \"temperature\": 0.7,\n",
    "}"
   ],
   "metadata": {},
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ppo_config = PPOConfig(**ppo_config) #mini_batch_size"
   ],
   "metadata": {},
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ppo_config.response_length=256"
   ],
   "metadata": {},
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from trl import PPOConfig\n",
    "ppo_config = PPOConfig()\n",
    "print(ppo_config.to_dict())"
   ],
   "metadata": {},
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'output_dir': 'trainer_output', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': 'no', 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': 'linear', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': 'trainer_output/runs/Jun01_07-11-02_63712dd7b2ad', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': None, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'trainer_output', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard', 'wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': None, 'hub_always_push': False, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False, 'average_tokens_across_devices': False, 'dataset_num_proc': None, 'num_mini_batches': 1, 'total_episodes': None, 'local_rollout_forward_batch_size': 64, 'num_sample_generations': 10, 'response_length': 53, 'stop_token': '<STOP_TOKEN>', 'stop_token_id': None, 'temperature': 0.7, 'missing_eos_penalty': None, 'sft_model_path': 'EleutherAI/pythia-160m', 'world_size': None, 'num_total_batches': None, 'micro_batch_size': None, 'local_batch_size': None, 'batch_size': None, 'local_mini_batch_size': None, 'mini_batch_size': None, 'exp_name': 'ppo_config', 'reward_model_path': 'EleutherAI/pythia-160m', 'model_adapter_name': None, 'ref_adapter_name': None, 'num_ppo_epochs': 4, 'whiten_rewards': False, 'kl_coef': 0.05, 'kl_estimator': 'k1', 'cliprange': 0.2, 'vf_coef': 0.1, 'cliprange_value': 0.2, 'gamma': 1.0, 'lam': 0.95, 'ds3_gather_for_generation': True}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class GPTRewardModel(torch.nn.Module):\n",
    "  base_model_prefix = \"backbone\"\n",
    "  def __init__(self, model_path):\n",
    "    super().__init__()\n",
    "    self.backbone = AutoModel.from_pretrained(model_path)\n",
    "    self.v_head = torch.nn.Linear(self.backbone.config.hidden_size, 1)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, **kwargs):\n",
    "    out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    hidden = out.last_hidden_state\n",
    "    last_token_index = attention_mask.sum(dim=1) - 1\n",
    "    batch_idx = torch.arange(input_ids.size(0), device=input_ids.device)\n",
    "    pooled = hidden[batch_idx, last_token_index]\n",
    "    logits = self.v_head(pooled)\n",
    "    return logits.view(-1)\n",
    "\n",
    "  def score(self, hidden_states, attention_mask=None, **kwargs):\n",
    "    return self.v_head(hidden_states).squeeze(-1)\n",
    "\n",
    "  @property\n",
    "  def config(self):\n",
    "    return self.backbone.config  # or self.backbone.model.config if needed"
   ],
   "metadata": {},
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "rm_model = GPTRewardModel(model_path=\"/content/drive/MyDrive/rm_backbone\")\n",
    "v_head_weights = torch.load(\"/content/drive/MyDrive/rm_backbone/v_head.bin\", map_location=\"cpu\")\n",
    "rm_model.v_head.load_state_dict(v_head_weights)\n",
    "rm_model.eval()\n",
    "for param in rm_model.parameters():\n",
    "    param.requires_grad = False"
   ],
   "metadata": {},
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "\n",
    "class CriticWithScore(AutoModelForCausalLMWithValueHead):\n",
    "  def score(self, hidden_states):\n",
    "    return self.v_head(hidden_states).squeeze(-1)\n",
    "\n",
    "  def forward(self, *args, **kwargs):\n",
    "    kwargs.setdefault(\"output_hidden_states\", True)\n",
    "    kwargs.setdefault(\"return_dict\", True)\n",
    "\n",
    "    # 1차 호출\n",
    "    outputs = super().forward(*args, **kwargs)\n",
    "\n",
    "    # hidden_states 없으면 수동으로 backbone에서 추출\n",
    "    if not hasattr(outputs, 'hidden_states'):\n",
    "      backbone = getattr(self, self.base_model_prefix)\n",
    "      transformer_outputs = backbone(*args, **kwargs)\n",
    "      hidden_states = transformer_outputs.hidden_states\n",
    "\n",
    "      outputs = CausalLMOutputWithCrossAttentions(\n",
    "          loss=outputs.loss if hasattr(outputs, \"loss\") else None,\n",
    "          logits=outputs.logits,\n",
    "          past_key_values=outputs.past_key_values if hasattr(outputs, \"past_key_values\") else None,\n",
    "          hidden_states=hidden_states,\n",
    "          attentions=outputs.attentions if hasattr(outputs, \"attentions\") else None,\n",
    "          cross_attentions=None,\n",
    "      )\n",
    "\n",
    "    return outputs"
   ],
   "metadata": {},
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_name=\"/content/drive/MyDrive/merged_model\"\n",
    "\n",
    "actor_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "#critic_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)\n",
    "critic_model = CriticWithScore.from_pretrained(model_name)\n",
    "\n",
    "ref_model = copy.deepcopy(actor_model)\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False"
   ],
   "metadata": {},
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "actor_model = copy.deepcopy(actor_model)"
   ],
   "metadata": {},
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "lora_config1 = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "actor_model = get_peft_model(actor_model, lora_config1)\n",
    "actor_model.print_trainable_parameters()"
   ],
   "metadata": {},
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trainable params: 8,110,080 || all params: 1,170,666,240 || trainable%: 0.6928\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for name, param in actor_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.shape}\")"
   ],
   "metadata": {},
   "execution_count": 83,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.1.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.1.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.1.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.1.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.2.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.2.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.2.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.2.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.3.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.3.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.3.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.3.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.4.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.4.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.4.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.4.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.5.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.5.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.5.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.5.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.6.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.6.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.6.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.6.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.7.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.7.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.7.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.7.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.8.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.8.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.8.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.8.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.9.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.9.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.9.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.9.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.10.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.10.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.10.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.10.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.11.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.11.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.11.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.11.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.12.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.12.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.12.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.12.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.12.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.12.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.13.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.13.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.13.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.13.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.13.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.13.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.14.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.14.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.14.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.14.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.14.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.14.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.15.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.15.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.15.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.15.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.15.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.15.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.16.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.16.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.16.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.16.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.16.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.16.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.17.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.17.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.17.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.17.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.17.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.17.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.18.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.18.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.18.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.18.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.18.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.18.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.19.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.19.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.19.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.19.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.19.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.19.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.20.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.20.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.20.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.20.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.20.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.20.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.21.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.21.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.21.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.21.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.21.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.21.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.22.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.22.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.22.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.22.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.22.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.22.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.23.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.23.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.transformer.h.23.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.transformer.h.23.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.transformer.h.23.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.transformer.h.23.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "lora_config2 = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "critic_model = get_peft_model(critic_model, lora_config2)\n",
    "critic_model.print_trainable_parameters()"
   ],
   "metadata": {},
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trainable params: 8,110,080 || all params: 1,170,668,161 || trainable%: 0.6928\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for name, param in critic_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.shape}\")"
   ],
   "metadata": {},
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "base_model.model.pretrained_model.transformer.h.0.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.0.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.0.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.0.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.0.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.0.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.1.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.1.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.1.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.1.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.1.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.1.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.2.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.2.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.2.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.2.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.2.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.2.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.3.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.3.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.3.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.3.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.3.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.3.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.4.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.4.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.4.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.4.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.4.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.4.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.5.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.5.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.5.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.5.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.5.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.5.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.6.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.6.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.6.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.6.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.6.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.6.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.7.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.7.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.7.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.7.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.7.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.7.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.8.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.8.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.8.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.8.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.8.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.8.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.9.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.9.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.9.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.9.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.9.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.9.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.10.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.10.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.10.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.10.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.10.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.10.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.11.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.11.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.11.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.11.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.11.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.11.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.12.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.12.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.12.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.12.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.12.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.12.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.13.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.13.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.13.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.13.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.13.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.13.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.14.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.14.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.14.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.14.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.14.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.14.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.15.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.15.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.15.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.15.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.15.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.15.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.16.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.16.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.16.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.16.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.16.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.16.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.17.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.17.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.17.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.17.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.17.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.17.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.18.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.18.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.18.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.18.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.18.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.18.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.19.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.19.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.19.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.19.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.19.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.19.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.20.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.20.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.20.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.20.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.20.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.20.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.21.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.21.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.21.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.21.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.21.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.21.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.22.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.22.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.22.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.22.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.22.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.22.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.23.attn.c_attn.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.23.attn.c_attn.lora_B.default.weight: torch.Size([5760, 16])\n",
      "base_model.model.pretrained_model.transformer.h.23.attn.c_proj.lora_A.default.weight: torch.Size([16, 1920])\n",
      "base_model.model.pretrained_model.transformer.h.23.attn.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n",
      "base_model.model.pretrained_model.transformer.h.23.mlp.c_proj.lora_A.default.weight: torch.Size([16, 7680])\n",
      "base_model.model.pretrained_model.transformer.h.23.mlp.c_proj.lora_B.default.weight: torch.Size([1920, 16])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for name, param in critic_model.named_parameters():\n",
    "    if \"v_head\" in name:\n",
    "        param.requires_grad = True\n",
    "        print(f\"Trainable: {name}\")"
   ],
   "metadata": {},
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trainable: base_model.model.v_head.summary.weight\n",
      "Trainable: base_model.model.v_head.summary.bias\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "critic_model.base_model_prefix = \"pretrained_model\""
   ],
   "metadata": {},
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "metadata": {},
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_parquet(\"hf://datasets/royboy0416/ko-alpaca/data/ko_alpaca_data.snappy.parquet\")\n",
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "\n",
    "def format_prompt(example):\n",
    "    if example[\"input\"]:\n",
    "        prompt = f\"{example['instruction']}\\n{example['input']}\"\n",
    "    else:\n",
    "        prompt = example[\"instruction\"]\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": example[\"output\"]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_prompt)"
   ],
   "metadata": {},
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/49620 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e26b76d3e824c3794ff4a5d114d7f85"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class PPODataset(Dataset):\n",
    "  def __init__(self, data, tokenizer, max_length=1024):\n",
    "    self.data = data\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_length = max_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    prompt = self.data[idx][\"prompt\"]\n",
    "    PROMPT_TEMPLATE = \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    prompt = PROMPT_TEMPLATE.format(prompt=prompt)\n",
    "\n",
    "    tokenized = self.tokenizer(prompt, padding = 'longest', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "        #\"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n",
    "    }"
   ],
   "metadata": {},
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ppo_train=PPODataset(dataset, tokenizer)"
   ],
   "metadata": {},
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "clean_data = []\n",
    "for i in range(len(ppo_train)):\n",
    "    sample = ppo_train[i]\n",
    "    clean_data.append({\n",
    "        \"input_ids\": [int(x) for x in sample[\"input_ids\"].tolist()],\n",
    "    })\n",
    "\n",
    "ppo_train_dataset = datasets.Dataset.from_list(clean_data)\n",
    "ppo_train_dataset.set_format(type=\"torch\")"
   ],
   "metadata": {},
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "split_dataset = ppo_train_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "ppo_train = split_dataset[\"test\"]\n",
    "ppo_test = split_dataset['train']"
   ],
   "metadata": {},
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(actor_model)"
   ],
   "metadata": {},
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(51200, 1920)\n",
      "        (wpe): Embedding(1024, 1920)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-23): 24 x GPT2Block(\n",
      "            (ln_1): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=5760, nx=1920)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1920, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=5760, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=1920, nx=1920)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1920, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1920, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D(nf=7680, nx=1920)\n",
      "              (c_proj): lora.Linear(\n",
      "                (base_layer): Conv1D(nf=1920, nx=7680)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=7680, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1920, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1920, out_features=51200, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "actor_model.base_model.model.generation_config"
   ],
   "metadata": {},
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 8,\n",
       "  \"pad_token_id\": 8\n",
       "}"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "actor_model.generation_config = GenerationConfig(**generation_kwargs)\n",
    "actor_model.base_model.generation_config = GenerationConfig(**generation_kwargs)\n",
    "actor_model.base_model.model.generation_config = GenerationConfig(**generation_kwargs)"
   ],
   "metadata": {},
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = PPOTrainer(\n",
    "    args=ppo_config,                      # PPOConfig instance\n",
    "    processing_class=tokenizer,          # Tokenizer\n",
    "    model=actor_model,                   # LoRA-wrapped actor model\n",
    "    ref_model=ref_model,                 # Frozen reference model\n",
    "    reward_model=rm_model,           # Reward model (can be a callable or model)\n",
    "    train_dataset=ppo_train,     # Your training dataset (must support __len__ and __getitem__)\n",
    "    value_model=critic_model,            # Critic model (with ValueHead)\n",
    "    data_collator=data_collator,        # HuggingFace-style data collator\n",
    "    eval_dataset=datasets.Dataset.from_list([ppo_train[0],ppo_train[1]])\n",
    "    )"
   ],
   "metadata": {},
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {},
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.eval_dataset = None\n",
    "trainer.eval_dataloader = None\n",
    "trainer.generate_completions = lambda *args, **kwargs: []"
   ],
   "metadata": {},
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ppo_config.num_sample_generations"
   ],
   "metadata": {},
   "execution_count": 69,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {},
   "execution_count": 44,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "===training policy===\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'no_repeat_ngram_size': 4, 'pad_token_id': 3}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='466' max='466' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [466/466 4:15:20, Epoch 3/3.0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "평가"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard"
   ],
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%tensorboard --logdir /content/drive/MyDrive/PPO/logs"
   ],
   "metadata": {},
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "        (async () => {\n",
       "            const url = new URL(await google.colab.kernel.proxyPort(6007, {'cache': true}));\n",
       "            url.searchParams.set('tensorboardColab', 'true');\n",
       "            const iframe = document.createElement('iframe');\n",
       "            iframe.src = url;\n",
       "            iframe.setAttribute('width', '100%');\n",
       "            iframe.setAttribute('height', '800');\n",
       "            iframe.setAttribute('frameborder', 0);\n",
       "            document.body.appendChild(iframe);\n",
       "        })();\n",
       "    "
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. policy loss와 value loss는 수렴하였다. 조심스러운 것이 value loss 자체는 reward regression 형태와 비슷하기 때문에 수렴하기 쉽고 따라서 advantage가 줄어듬에 따라 policy loss도 감소한 것이 아닐까 싶다. 따라서 단순 수렴만으로 policy 개선을 판단하기 어려우며 reward와 advantage 경향성을 확인해야한다.\n",
    "2. KL을 보면 policy update가 학습초기에 크다가 이후에는 policy update가 sft model과 stable하게 수렴하는 것을 보여준다.\n",
    "3. non-scroe reward는 rm의 reward 결과를 보여주는데 모델이 rm의 reward를 안정적으로 따라간 것으로 보인다. 다만 rm이 상대적 크기가 중요하더라도 reward가 음수 영역에서 나오는 것은 rm 자체의 평균 reward를 확인해 봐야 할 것같다. 필요시 reward normalization 이나 score shifting을 고려할 수 있다.\n",
    "4. entropy 는 증가하는 경향을 보이는데 결정적인 explotiation 영역에서 탐색적인 exploration 영역으로 진행되었다. 엔트로피는 actor model이 좋은 policy 방향으로 학습되면서 감소하는 것이 일반적인데 rm 기준을 beam과 greedy 샘플의 비교를 넣은 것과 policy grad 대상이 actor_model의 sample generate 기준이었기 때문일 것 같다. 특히 n-gram repeat 방지는 모델에게 탐색을 더 강요한다.\n",
    "5. entropy 증가의 다른 이유로는 rm sharpness의 부족이 우려된다. gold와 beam, greedy의 비교 정확도는 90 이상이 나왔지만 score 자체의 scale차이가 크지 않을 수 있다. 확인이 필요하다. rm dataset 품질을 올리거나 직접적으로 rm score scaling을 해서 reward의 차이와 영향을 키울 수 있다.\n",
    "6. rlhf reward, score를 보면 actor_model이 reward를 잘 따라가고 있음을 확인할 수 있다. 다만 reward가 증가해도 entropy가 증가하는 것은 모델이 보상을 받기 위해 다양한 전략을 탐색하는 것으로 판단할 수 있다.\n",
    "7. 추가적으로 rm을 마지막 유효토큰 기준으로 계산한 것(원본코드는 마지막 토큰 제외 평균)과 kl coeff, gae, ppo coeff 등 실험조건의 영향을 비교해 볼 수 있다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model_name=\"/content/drive/MyDrive/merged_model\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "actor_model = PeftModel.from_pretrained(base_model, \"/content/drive/MyDrive/PPO/checkpoint-466\")"
   ],
   "metadata": {},
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "actor_model = actor_model.merge_and_unload()\n",
    "\n",
    "actor_model.save_pretrained(\"/content/drive/MyDrive/PPO/PPO_result\")"
   ],
   "metadata": {},
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "greedy_generation_args = dict(\n",
    "    do_sample=False,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=1,\n",
    "    repetition_penalty=1.0,\n",
    "    no_repeat_ngram_size=0,\n",
    "    batch_size=16,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "beam_generation_args = dict(\n",
    "    num_beams=5,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=1,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    top_k=40,\n",
    "    early_stopping=True,\n",
    "    temperature = 0.7\n",
    ")"
   ],
   "metadata": {},
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "generator = pipeline('text-generation', model='/content/drive/MyDrive/merged_model', tokenizer=tokenizer)\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [ dataset['prompt'][30], dataset['prompt'][500] ,dataset['prompt'][1100], dataset['prompt'][2000]]\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **beam_generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ],
   "metadata": {},
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "식물의 세포 호흡 과정을 설명하십시오.\n",
      "\n",
      "### Response(응답):식물은 광합성을 통해 이산화탄소를 흡수하고 산소를 방출하여 에너지를 생성합니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "로미오와 줄리엣에 나오는 로미오의 성격을 묘사하십시오.\n",
      "\n",
      "### Response(응답):로미오는 사랑스럽고 친절하며 로맨틱한 성격을 가지고 있습니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "주어진 좌표를 기준으로 가장 가까운 공항을 구하십시오.\n",
      "40.728157, -73.794853\n",
      "\n",
      "### Response(응답):입력된 좌표 기준 가장 가까운 공항은 40.728157과 73.7944853의 중간 지점인 미국 하와이 호놀룰루입니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "동물의 서식지를 시뮬레이션하는 프로그램을 설계하십시오.\n",
      "\n",
      "### Response(응답):동물의 서식지 시뮬레이션 프로그램을 설계하기 위해서는, 해당 지역의 환경과 생태계, 생물 다양성 등을 고려해야 합니다. 이를 위해 데이터 수집, 모델링 및 시뮬레이션을 수행할 수 있는 알고리즘이 필요합니다.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "generator = pipeline('text-generation', model='/content/drive/MyDrive/PPO/PPO_result', tokenizer=tokenizer)\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [ dataset['prompt'][30], dataset['prompt'][500] ,dataset['prompt'][1100], dataset['prompt'][2000]]\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **beam_generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ],
   "metadata": {},
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "식물의 세포 호흡 과정을 설명하십시오.\n",
      "\n",
      "### Response(응답):식물은 광합성을 통해 이산화탄소와 물을 흡수하고 산소를 방출하여 에너지를 생산합니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "로미오와 줄리엣에 나오는 로미오의 성격을 묘사하십시오.\n",
      "\n",
      "### Response(응답):로미오는 강하고 용감하며, 로맨틱한 성격입니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "주어진 좌표를 기준으로 가장 가까운 공항을 구하십시오.\n",
      "40.728157, -73.794853\n",
      "\n",
      "### Response(응답):입력된 좌표 기준의 가장 가까운 공항은 40.728157과 73.7944853 사이에 위치한 미국 하와이 호놀룰루입니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "동물의 서식지를 시뮬레이션하는 프로그램을 설계하십시오.\n",
      "\n",
      "### Response(응답):동물 서식지 시뮬레이션 프로그램을 설계하기 위해서는 동물의 종류, 크기, 지리적 위치, 생태계 및 생물 다양성 등 다양한 요소를 고려해야 합니다. 이러한 요소들을 고려하여 실제 환경과 유사한 환경을 구축하고, 각 동물에 대한 데이터를 수집하고 이를 분석하여 모델링할 수 있는 알고리즘을 개발해야 합니다.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "generator = pipeline('text-generation', model='/content/drive/MyDrive/merged_model', tokenizer=tokenizer)\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [ dataset['prompt'][30], dataset['prompt'][500] ,dataset['prompt'][1100], dataset['prompt'][2000]]\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **greedy_generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ],
   "metadata": {},
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "식물의 세포 호흡 과정을 설명하십시오.\n",
      "\n",
      "### Response(응답)::식물은 광합성을 통해 이산화탄소와 물을 흡수합니다. 이산화탄소는 산소와 결합하여 산소를 생성하고, 물은 이산화탄소와 물로 분해됩니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "로미오와 줄리엣에 나오는 로미오의 성격을 묘사하십시오.\n",
      "\n",
      "### Response(응답)::로미오는 사랑스럽고 용감하며 로맨틱한 성격을 가지고 있습니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "주어진 좌표를 기준으로 가장 가까운 공항을 구하십시오.\n",
      "40.728157, -73.794853\n",
      "\n",
      "### Response(응답):40.728157, -73.794853의 가장 가까운 공항은 Airport Center입니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "동물의 서식지를 시뮬레이션하는 프로그램을 설계하십시오.\n",
      "\n",
      "### Response(응답)::동물의 서식지를 시뮬레이션하는 프로그램을 설계하는 방법은, 먼저, 동물들의 서식지를 조사하고, 그 후 각 동물들의 서식지를 기반으로 시뮬레이션을 수행합니다.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "generator = pipeline('text-generation', model='/content/drive/MyDrive/PPO/PPO_result', tokenizer=tokenizer)\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [ dataset['prompt'][30], dataset['prompt'][500] ,dataset['prompt'][1100], dataset['prompt'][2000]]\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **greedy_generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ],
   "metadata": {},
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "식물의 세포 호흡 과정을 설명하십시오.\n",
      "\n",
      "### Response(응답)::세포 호흡은 세포가 에너지를 생성하기 위해 산소를 사용하는 과정입니다. 이 과정에서 세포는 산소를 흡수하고 이산화탄소를 방출합니다. 이 과정에서 세포는 산소를 흡수하고 이산화탄소를 방출합니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "로미오와 줄리엣에 나오는 로미오의 성격을 묘사하십시오.\n",
      "\n",
      "### Response(응답):은근하고 친절하며 로맨틱한 성격입니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "주어진 좌표를 기준으로 가장 가까운 공항을 구하십시오.\n",
      "40.728157, -73.794853\n",
      "\n",
      "### Response(응답):40.728157, -73.794853의 거리는 약 1,092km입니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "동물의 서식지를 시뮬레이션하는 프로그램을 설계하십시오.\n",
      "\n",
      "### Response(응답)::동물의 서식지를 시뮬레이션하는 프로그램을 설계하는 것은 매우 복잡합니다. 예를 들어, 어떤 동물들이 서식하는지, 어떤 종류의 동물이 살고 있는지, 그리고 어떤 종류의 동물이 가장 많이 서식하는지에 대한 정보를 수집해야 합니다. 이러한 정보를 기반으로 시뮬레이션 프로그램을 설계하고 실행할 수 있습니다.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "generator1 = pipeline('text-generation', model='/content/drive/MyDrive/merged_model', tokenizer=tokenizer)\n",
    "generator2 = pipeline('text-generation', model='/content/drive/MyDrive/PPO/PPO_result', tokenizer=tokenizer)"
   ],
   "metadata": {},
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = random.sample(dataset['prompt'], 4)\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator1(list_prompt, **beam_generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))\n",
    "print('\\n', 'which one is better?')\n",
    "list_result = generator2(list_prompt, **beam_generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ],
   "metadata": {},
   "execution_count": 42,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "전통적인 비즈니스 형태와 현대적인 비즈니스 방식을 비교하고 대조합니다.\n",
      "\n",
      "### Response(응답):전통적인 비즈니스와 현대적인 비즈니스의 차이점은 다음과 같습니다: 전통 비즈니스는 고객 서비스, 제품 및 서비스 제공에 중점을 둡니다. 반면, 현대적인 비즈니스는 창의성과 혁신에 중점을 둔다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "React Native와 Flutter의 기능을 비교하고 대조합니다.\n",
      "\n",
      "### Response(응답):Flutter는 자동화된 프로그래밍 언어이며, React Natives는 대화형 프로그래밍 언어입니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "다음 맛을 설명하는 고유 단어 다섯 개를 생성하세요.\n",
      "Sour\n",
      "\n",
      "### Response(응답):달콤한, 신맛, 쓴맛, 짠맛\n",
      "\n",
      "### Instruction(명령어):\n",
      "기후 변화에 대처하기 위한 계획을 작성하세요.\n",
      "\n",
      "### Response(응답):기후 변화에 대응하기 위한 계획은 다음과 같습니다: 에너지 절약, 친환경 교통수단 사용, 재활용 및 재사용 촉진 등입니다.\n",
      "\n",
      " which one is better?\n",
      "\n",
      "### Instruction(명령어):\n",
      "전통적인 비즈니스 형태와 현대적인 비즈니스 방식을 비교하고 대조합니다.\n",
      "\n",
      "### Response(응답):전통적인 비즈니스와 현대적인 비즈니스의 차이점은 다음과 같습니다: 전통 비즈니스는 대규모 자본을 필요로 하지만, 현대적인 비즈니스는 소규모 자본으로도 창업이 가능합니다. 또한 전통 비즈니스는 더 높은 수익을 창출할 수 있지만, 현대적인 비즈니스에서는 더 적은 비용으로 더 많은 고객을 유치할 수 있습니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "React Native와 Flutter의 기능을 비교하고 대조합니다.\n",
      "\n",
      "### Response(응답):Flutter는 더 직관적인 인터페이스를 제공하지만, React Natives는 좀 더 개인화된 경험을 제공합니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "다음 맛을 설명하는 고유 단어 다섯 개를 생성하세요.\n",
      "Sour\n",
      "\n",
      "### Response(응답):\"달콤한\", \"신선한\", \"단단한\", \"담백한\", \"고소한\"입니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "기후 변화에 대처하기 위한 계획을 작성하세요.\n",
      "\n",
      "### Response(응답):온실 가스 배출량을 줄이기 위해 대중교통을 이용하고, 에너지 효율적인 가전제품을 사용하며, 탄소 중립적인 식습관을 유지하는 등의 방법으로 기후 변화에 대처할 수 있습니다.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "정성 평가 결과, PPO가 약 7:3 정도의 비율로 SFT보다 더 만족스러운 응답을 생성하는 것으로 판단되었으며, 두 모델의 응답이 유사한 경우도 상당수 확인되었다. 다만 외부 데이터 자체의 오류나 부정확한 프롬프트가 포함된 경우에는 두 모델 모두 유사한 유형의 오답을 내는 경향이 있었다.\n",
    "\n",
    "PPO 모델의 추가 개선 방향으로는 엔트로피 증가를 제어하여 불필요한 탐색을 억제하는 전략이 고려될 수 있다. 또한 Reward Model의 설계 측면에서도, 현재는 마지막 유효 토큰 기준으로 reward를 계산하고 있으나, 이를 전체 유효 토큰에 대한 평균 혹은 다른 방식으로 확장함으로써 reward의 안정성을 높일 여지가 있다.\n",
    "\n",
    "궁극적으로 PPO의 성능 개선은 RM의 품질에 의해 결정되며, 현재 구성된 RM 학습 데이터셋은 SFT에 사용되지 않은 prompt에 대해 gold, beam, greedy 응답을 비교하여 구성되어 있어, 추가적인 성능 향상을 유도하기 어려운 한계가 있다.\n",
    "\n",
    "따라서 향후 개선 방향으로는 RM 데이터셋을 보다 정교하게 구성하는 것이 필요하며, 이를 위해 외부의 우수한 모델 또는 웹 검색 등의 외부 지식 접근이 가능한 모델을 활용하여 보다 신뢰도 높은 응답을 확보하고, 이를 reward 기준으로 사용하는 방식이 효과적일 것으로 기대된다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "trial"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "for batch in trainer.dataloader:\n",
    "  query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "  with torch.no_grad():\n",
    "    response_tensors = actor_model.generate(query_tensors,attention_mask=batch['attention_mask'], **generation_kwargs)\n",
    "\n",
    "  response_list=tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "  rm_tokens = [torch.tensor(tokenizer.encode(s)) for s in response_list]\n",
    "  rm_tokens_pad = pad_sequence(rm_tokens, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "  rm_attention_mask = (rm_tokens_pad != tokenizer.pad_token_id).long()\n",
    "\n",
    "  rm_model.to(device)\n",
    "  with torch.no_grad():\n",
    "    rewards = rm_model(rm_tokens_pad.to(device), rm_attention_mask.to(device))\n",
    "  rm_model.to(\"cpu\")\n",
    "  del rm_tokens, rm_tokens_pad, rm_attention_mask\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  # for i in len(rewards):\n",
    "  #   query = query_tensors[i]\n",
    "  #   reward = rewards[i]\n",
    "  #   response = response_tensors[i]\n",
    "  #   real_response = response[query_tensors.shape[1]:]\n",
    "\n",
    "  real_response = response_tensors[:, query_tensors.shape[1]:]\n",
    "\n",
    "  stats = trainer.batched_step(query_tensors, real_response , rewards)\n",
    "\n",
    "  print(f\"[Step 1] KL={stats['objective/kl']:.4f}, Loss={stats['objective/loss']:.4f}\")\n",
    "  del query_tensors, response_tensors, real_response, rewards\n",
    "  torch.cuda.empty_cache()\n",
    "  break"
   ],
   "metadata": {},
   "execution_count": 195,
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'PPOTrainer' object has no attribute 'batched_step'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-5fe61e2e3d64>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mreal_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatched_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_response\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Step 1] KL={stats['objective/kl']:.4f}, Loss={stats['objective/loss']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PPOTrainer' object has no attribute 'batched_step'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from trl.core import logprobs_from_logits\n",
    "\n",
    "def compute_reward(prompt_input_ids, response_input_ids, model, ref_model, reward_model):\n",
    "  full_input = torch.cat([prompt_input_ids, response_input_ids], dim=1)\n",
    "  model_outputs = model(full_input)\n",
    "  ref_outputs = ref_model(full_input)\n",
    "\n",
    "  logprobs = logprobs_from_logits(model_outputs.logits, full_input)\n",
    "  ref_logprobs = logprobs_from_logits(ref_outputs.logits, full_input)\n",
    "  kl = logprobs - ref_logprobs\n",
    "\n",
    "  score = reward_model(full_input)\n",
    "  reward = -kl.mean(dim=-1) + score\n",
    "  return reward"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from math import ceil\n",
    "\n",
    "n_data = len(dataset)\n",
    "min_steps = ceil(n_data / ppo_config.batch_size)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# model init\n",
    "    \"sft_model_path\": \"/content/drive/MyDrive/merged_model\","
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class CriticWithScore(AutoModelForCausalLMWithValueHead):\n",
    "  def score(self, hidden_states):\n",
    "    return self.v_head(hidden_states).squeeze(-1)\n",
    "  def forward(self, *args, **kwargs):\n",
    "        kwargs.setdefault(\"output_hidden_states\", True)\n",
    "        kwargs.setdefault(\"return_dict\", True)\n",
    "        return super().forward(*args, **kwargs)\n",
    "\n",
    "class CriticWithScore(AutoModelForCausalLMWithValueHead):\n",
    "    def score(self, hidden_states):\n",
    "        return self.v_head(hidden_states).squeeze(-1)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        kwargs.setdefault(\"output_hidden_states\", True)\n",
    "        kwargs.setdefault(\"return_dict\", True)\n",
    "        outputs = super().forward(*args, **kwargs)\n",
    "\n",
    "        # hidden_states는 transformer의 결과에만 있음\n",
    "        # outputs.hidden_states가 없으면 여기서 직접 전달\n",
    "        if not hasattr(outputs, 'hidden_states'):\n",
    "            # transformer에 직접 접근해서 hidden_states 추출\n",
    "            transformer_outputs = self.transformer(\n",
    "                *args,\n",
    "                **kwargs,\n",
    "            )\n",
    "            hidden_states = transformer_outputs.hidden_states\n",
    "            outputs = CausalLMOutputWithCrossAttentions(\n",
    "                loss=outputs.loss if hasattr(outputs, \"loss\") else None,\n",
    "                logits=outputs.logits,\n",
    "                past_key_values=outputs.past_key_values if hasattr(outputs, \"past_key_values\") else None,\n",
    "                hidden_states=hidden_states,\n",
    "                attentions=outputs.attentions if hasattr(outputs, \"attentions\") else None,\n",
    "                cross_attentions=None,\n",
    "            )\n",
    "\n",
    "        return outputs"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Inside trainer:\")\n",
    "print(\"reward_model =\", type(trainer.reward_model))\n",
    "print(\"value_model =\", type(trainer.value_model))\n",
    "print(\"Has trainer.reward_model.score?\", hasattr(trainer.reward_model, \"score\"))\n",
    "print(\"Has trainer.value_model.score?\", hasattr(trainer.value_model, \"score\"))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for batch in trainer.dataloader:\n",
    "    print(batch)\n",
    "    print(type(batch))\n",
    "    break"
   ],
   "metadata": {},
   "execution_count": 42,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': tensor([[30132, 42872, 33313,  ...,     3,     3,     3],\n",
      "        [30132, 42872, 33313,  ...,     3,     3,     3],\n",
      "        [30132, 42872, 33313,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [30132, 42872, 33313,  ...,     3,     3,     3],\n",
      "        [30132, 42872, 33313,  ...,     3,     3,     3],\n",
      "        [30132, 42872, 33313,  ...,     3,     3,     3]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer.padding_side = \"left\"  # generation 시 필수\n",
    "actor_model.config.pad_token_id\n",
    "tokenizer.encode(\"<pad>\")\n",
    "tokenizer.decode([8])"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}