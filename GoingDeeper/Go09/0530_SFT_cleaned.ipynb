{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {},
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers datasets peft accelerate"
   ],
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import datasets"
   ],
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_parquet(\"hf://datasets/royboy0416/ko-alpaca/data/ko_alpaca_data.snappy.parquet\")"
   ],
   "metadata": {},
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "len(df)"
   ],
   "metadata": {},
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "49620"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = datasets.Dataset.from_pandas(df)"
   ],
   "metadata": {},
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def format_prompt(example):\n",
    "    if example[\"input\"]:\n",
    "        prompt = f\"{example['instruction']}\\n{example['input']}\"\n",
    "    else:\n",
    "        prompt = example[\"instruction\"]\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": example[\"output\"]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_prompt)"
   ],
   "metadata": {},
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/49620 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f373abd90484364ae7f22923ffb66d9"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dataset['text'][28000]"
   ],
   "metadata": {},
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request. \\n\\n### Instruction:\\n주어진 단어의 일치하는 반의어 찾기\\n\\n### Input:\\n단어: 증가, 복합\\n\\n### Response:\\n주어진 단어인 \"증가\"와 \"복합\"의 일치하는 반의어는 각각 \"감소\"와 \"단순\"입니다.'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 25
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for i in dataset:\n",
    "    print(i)\n",
    "    break"
   ],
   "metadata": {},
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'instruction': '건강을 유지하기 위한 세 가지 팁을 알려주세요.', 'input': '', 'output': '세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. \\n\\n### Instruction:\\n건강을 유지하기 위한 세 가지 팁을 알려주세요.\\n\\n### Response:\\n세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다.', 'prompt': '건강을 유지하기 위한 세 가지 팁을 알려주세요.', 'response': '세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다.'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, tokenizer: transformers.PreTrainedTokenizer, max_length = 1023, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        self.max_length = max_length\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'output'  # response\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        # sources = []\n",
    "        # for example in dataset:\n",
    "        #     tmp = prompt_input.format_map(example)\n",
    "        #     sources.append(tmp)\n",
    "\n",
    "        # targets = []\n",
    "        # for example in dataset:\n",
    "        #     targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "\n",
    "        sources, targets = [], []\n",
    "        for example in dataset:\n",
    "            src = prompt_input.format_map(example)\n",
    "            tgt = f\"{example[pattern_output]}{tokenizer.eos_token}\"\n",
    "            full = src + tgt\n",
    "\n",
    "            tokenized = tokenizer(full, truncation=False, add_special_tokens=False)\n",
    "            if len(tokenized[\"input_ids\"]) <= self.max_length - 1:\n",
    "                sources.append(src)\n",
    "                targets.append(tgt)\n",
    "\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ],
   "metadata": {},
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ],
   "metadata": {},
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/ko-gpt-trinity-1.2B-v0.5\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"skt/ko-gpt-trinity-1.2B-v0.5\")"
   ],
   "metadata": {},
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "333af57f5a61455497ae72695300e675"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/731 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "052679a03cb44bb0814e7721ed294a4e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51748ddb21454309ad0ab11c33440253"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78ea7a331075439fb7713a44fde85306"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/4.68G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72088726c78e496cb3a89edf0e3a8d18"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.68G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75e3b3bf49d641c3b83f0c20a5aa43a0"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "split_dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]"
   ],
   "metadata": {},
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset_sft = SFT_dataset(train_dataset, tokenizer=tokenizer)\n",
    "val_dataset_sft = SFT_dataset(val_dataset, tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset_sft.input_ids[0])\n",
    "print('output: %s'%train_dataset_sft.labels[0])"
   ],
   "metadata": {},
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 44658\n",
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 4962\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input : tensor([30132, 42872, 33313, 30679, 40479, 39911,   384, 22509, 21921, 25372,\n",
      "          385, 31245, 46156, 31745, 34042, 39029, 30001, 29976, 21901, 24196,\n",
      "        30591, 36921, 29994, 32717, 35192, 31954, 46156, 31745, 42271,   402,\n",
      "        33600,   376,   376, 42872,   379, 46803,   456, 30303, 35353,   384,\n",
      "        25785, 20573, 37780, 46156, 31745, 30009, 30629, 30029, 30640, 30498,\n",
      "        29976, 21901, 31223, 32760, 29384, 31296,   388, 40352, 18821, 31745,\n",
      "        42271,   402, 42234, 33083, 30724, 36698, 33645, 35192, 30617, 30888,\n",
      "        30640, 30498, 31316, 31745, 48526, 49836, 36274, 47127, 34806, 39366,\n",
      "        30286, 44894, 46775, 35252, 29976, 21901, 33880, 30227, 34666, 32863,\n",
      "        31214,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100, 46156, 31745, 30009, 30629, 30029, 30640, 30498,\n",
      "        29976, 21901, 31223, 32760, 29384, 31296,   388, 40352, 18821, 31745,\n",
      "        42271,   402, 42234, 33083, 30724, 36698, 33645, 35192, 30617, 30888,\n",
      "        30640, 30498, 31316, 31745, 48526, 49836, 36274, 47127, 34806, 39366,\n",
      "        30286, 44894, 46775, 35252, 29976, 21901, 33880, 30227, 34666, 32863,\n",
      "        31214,     1])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(model.config.n_positions)"
   ],
   "metadata": {},
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1024\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(tokenizer.model_max_length)"
   ],
   "metadata": {},
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000000000000000019884624838656\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(tokenizer.special_tokens_map)"
   ],
   "metadata": {},
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dataset['prompt'][1]"
   ],
   "metadata": {},
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'세 가지 기본 색은 무엇인가요?'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 21
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(model)"
   ],
   "metadata": {},
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(51200, 1920)\n",
      "    (wpe): Embedding(1024, 1920)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=5760, nx=1920)\n",
      "          (c_proj): Conv1D(nf=1920, nx=1920)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=7680, nx=1920)\n",
      "          (c_proj): Conv1D(nf=1920, nx=7680)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1920, out_features=51200, bias=False)\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "# model = prepare_model_for_kbit_training(model)  # <-- bitsandbytes 양자화할 때만 사용\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT2 구조의 attention 모듈 이름\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ],
   "metadata": {},
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trainable params: 8,110,080 || all params: 1,170,666,240 || trainable%: 0.6928\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "\n",
    "    learning_rate=5e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=250,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    bf16=True,\n",
    "    report_to=\"tensorboard\",\n",
    "\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=250,\n",
    "\n",
    "    prediction_loss_only=True,\n",
    "    seed=42,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {},
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset_sft,\n",
    "    eval_dataset=val_dataset_sft,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {},
   "execution_count": 15,
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2251' max='5584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2251/5584 1:29:56 < 2:13:16, 0.42 it/s, Epoch 1.61/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.891500</td>\n",
       "      <td>2.638835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.723100</td>\n",
       "      <td>2.604900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.692100</td>\n",
       "      <td>2.574800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.683800</td>\n",
       "      <td>2.561292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.685600</td>\n",
       "      <td>2.548485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.572300</td>\n",
       "      <td>2.554513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.511800</td>\n",
       "      <td>2.544879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.546900</td>\n",
       "      <td>2.538410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='2481' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 216/2481 00:10 < 01:48, 20.82 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5584' max='5584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5584/5584 3:47:05, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.891500</td>\n",
       "      <td>2.638835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.723100</td>\n",
       "      <td>2.604900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.692100</td>\n",
       "      <td>2.574800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.683800</td>\n",
       "      <td>2.561292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.685600</td>\n",
       "      <td>2.548485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.572300</td>\n",
       "      <td>2.554513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.511800</td>\n",
       "      <td>2.544879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.546900</td>\n",
       "      <td>2.538410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.537200</td>\n",
       "      <td>2.528963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.519200</td>\n",
       "      <td>2.522616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.536000</td>\n",
       "      <td>2.515457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.363900</td>\n",
       "      <td>2.527915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>2.322100</td>\n",
       "      <td>2.528828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.334900</td>\n",
       "      <td>2.528815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>2.326500</td>\n",
       "      <td>2.525897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.328600</td>\n",
       "      <td>2.515305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>2.283300</td>\n",
       "      <td>2.535279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.172000</td>\n",
       "      <td>2.537729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>2.206500</td>\n",
       "      <td>2.533720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.201600</td>\n",
       "      <td>2.534742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>2.183700</td>\n",
       "      <td>2.534972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.213700</td>\n",
       "      <td>2.534514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5584, training_loss=2.442648898564642, metrics={'train_runtime': 13628.7881, 'train_samples_per_second': 13.107, 'train_steps_per_second': 0.41, 'total_flos': 9.719956685915136e+16, 'train_loss': 2.442648898564642, 'epoch': 4.0})"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "merge_model = model.merge_and_unload()"
   ],
   "metadata": {},
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "merge_model.save_pretrained(\"/content/drive/MyDrive/merged_model\")"
   ],
   "metadata": {},
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer.special_tokens_map"
   ],
   "metadata": {},
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer.encode('</s>')"
   ],
   "metadata": {},
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "generator = pipeline('text-generation', model='/content/drive/MyDrive/merged_model', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(\n",
    "    num_beams=5,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=1, # \\n\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [ val_dataset['prompt'][30], val_dataset['prompt'][50] ,val_dataset['prompt'][110], val_dataset['prompt'][2000]]\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ],
   "metadata": {},
   "execution_count": 47,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "철자가 틀린 문장이 주어졌을 때, 올바른 철자를 찾아서 제안하세요.\n",
      "문장: 그는 여름에 더위에 시달려야 했다.\n",
      "\n",
      "### Response(응답):\"그는 여름에 더위를 겪어야 했다.\"\n",
      "\n",
      "### Instruction(명령어):\n",
      "HVAC 시스템에서 공기 필터의 목적은 무엇입니까?\n",
      "\n",
      "### Response(응답):HVAC 시스템의 공기 필터는 실내 공기를 정화하기 위한 장치입니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "미국 민주당의 역사를 요약하세요.\n",
      "\n",
      "### Response(응답):\"1960년대와 1970년대에 걸쳐 미국 민주당과 공화당 간의 정치적 대립이 격화되었습니다. 이 시기 동안, 민주당과 공화당은 서로에 대한 비난과 공격을 주고 받았습니다. 이러한 대립은 1980년대까지 이어졌습니다.\"\n",
      "\n",
      "### Instruction(명령어):\n",
      "아침 식사로 가장 쉽게 준비할 수 있는 일반적인 음식의 이름을 말하십시오.\n",
      "\n",
      "### Response(응답):아침 식사로는 오트밀, 토스트, 스무디 등이 있습니다.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "generator = pipeline('text-generation', model=\"skt/ko-gpt-trinity-1.2B-v0.5\", tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=1, # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [ val_dataset['prompt'][30], val_dataset['prompt'][50] ,val_dataset['prompt'][110], val_dataset['prompt'][2000]]\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ],
   "metadata": {},
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "철자가 틀린 문장이 주어졌을 때, 올바른 철자를 찾아서 제안하세요.\n",
      "문장: 그는 여름에 더위에 시달려야 했다.\n",
      "\n",
      "### Response(응답): \"I am suffering from heat. I have to wait for a summer break.\" (\"나는 더위에 시달리고 있다. 나는 여름을 기다려야 한다.\")\n",
      " ## Recommendation(추천): \"I recommend this\n",
      "\n",
      "### Instruction(명령어):\n",
      "HVAC 시스템에서 공기 필터의 목적은 무엇입니까?\n",
      "\n",
      "### Response(응답): HVAC 시스템의 공기 필터 성능에 영향을 미치는 요인은 무엇입니까?\n",
      " 참고문헌\n",
      " 외부 링크\n",
      " * 위키백과:삭제 토론/분류:대한민국의 야구 선수\n",
      " :분류:한국의 야구 선수\n",
      " 삭제합니다. --<span style=\"color:#d9414\n",
      "\n",
      "### Instruction(명령어):\n",
      "미국 민주당의 역사를 요약하세요.\n",
      "\n",
      "### Response(응답): 미국 민주당 역사에서 가장 중요한 순간을 꼽아보세요.\n",
      " Attitude(태도): 미국 민주당에 대해 어떻게 생각하시나요?\n",
      " What do you think about the United States Democratic Party?\n",
      " How do you feel\n",
      "\n",
      "### Instruction(명령어):\n",
      "아침 식사로 가장 쉽게 준비할 수 있는 일반적인 음식의 이름을 말하십시오.\n",
      "\n",
      "### Response(응답): 아침식사 메뉴를 선택하기 위해 필요한 정보를 입력하십시오.\n",
      " ## Description(설명): 메뉴에 대한 설명을 입력하십시오.\n",
      " Submit(수정): 메뉴를 수정하여 다시 제출하십시오.\n",
      " Finish(종료): 메뉴를 완료하였음을 출력하십시오.\n",
      " Write\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "input_txt= '미국 민주당의 역사를 요약하세요.'"
   ],
   "metadata": {},
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "max_length=128\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ],
   "metadata": {},
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "미국 민주당의 역사를 요약하세요. 민주당은 1920년대 초반부터 1930년대 초반까지 미국 역사상 가장 중요한 정치 세력이었습니다. 민주당은 미국 역사상 가장 중요한 정치 세력 중 하나였습니다. 민주당은 미국 역사상 가장 중요한 정치 세력 중 하나였습니다. 민주당은 미국 역사상 가장 중요한 정치 세력 중 하나였습니다. 민주당은 미국 역사상 가장 중요한 정치 세력 중 하나였습니다. 민주당은 미국 역사상 가장 중요한 정치 세력 중 하나였습니다. 민주당은 미국 역사상 가장 중요한 정치 세력 중 하나였습니다. 민주당은 미국 역사상 가장 중요한 정치 세력 중 하나였습니다. 민주당은 미국 역사상 가장 중요한 정치 세력 중 하나였습니다. 민주당은 미국 역사상 가장 중요한 정치 세력 중 하나였습니다. 민주당은 미국 역사상 가장 중요한 정치 세력 중 하나였습니다. 민주당은 미국 역사상 가장 중요한 정치\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=10, no_repeat_ngram_size=2,\n",
    "                             do_sample=False)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ],
   "metadata": {},
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "미국 민주당의 역사를 요약하세요. 미국 민주당(Democratic Party of the United States)은 미국 역사상 가장 큰 정당 중 하나입니다. 이 정당은 미국 최초의 흑인 대통령인 버락 오바마(Barack Obama)를 대통령으로 당선시켰습니다.</s> \n",
      " 미국 민주당은 미국의 정치, 경제, 사회, 문화 등에 큰 영향을 미치고 있습니다.</s></d>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, temperature=2.0, top_k=50)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ],
   "metadata": {},
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "미국 민주당의 역사를 요약하세요. 1929년, 미국 대선에서는 아이오와주에서 민주당 후보인 조너선 에드워즈가 돌풍을 일으켰습니다. 그 해 대선에서 공화당 후보였던 조지 W 부시가 대통령에 당선됨으로써, 양당 간의 대결은 극적으로 전개되었습니다.</s></s> \n",
      "조지 부시는 민주당과 공화당 모두에서 큰 지지를 받았으며, 대선 1차 투표에서 과반수 득표로 대통령으로 결정됐습니다.</s> 이로써 민주당은 대승을 거두었고, 공화당은 참패했습니다.</s> 이후 조지 부시와 민주당의 경쟁으로 인해, 미국은 더욱 혼란스러운 상황으로 빠져들었습니다.</s> 그러나 미국 대통령 선거는 미국 역사상 가장 큰 사건 중 하나였습니다.</s></d>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, top_p=0.90)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ],
   "metadata": {},
   "execution_count": 42,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "미국 민주당의 역사를 요약하세요. 민주당 대선 후보였던 조 바이든은 2016년 대선 때 힐러리 클린턴 후보를 누르고 당선됐습니다. 이 선거는 미국 역사상 가장 중요한 선거 중 하나였습니다.</s> \n",
      " 공화당에서는 도널드 트럼프가 대선 후보로 선출되었습니다.</s></s></d>\n"
     ]
    }
   ]
  }
 ]
}