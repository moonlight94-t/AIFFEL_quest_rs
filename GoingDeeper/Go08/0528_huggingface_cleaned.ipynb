{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {},
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets peft accelerate"
      ],
      "metadata": {},
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4"
      ],
      "metadata": {},
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18.0\n",
            "1.26.4\n",
            "4.52.2\n",
            "2.14.4\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "import numpy\n",
        "import transformers\n",
        "import datasets\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(tensorflow.__version__)\n",
        "print(numpy.__version__)\n",
        "print(transformers.__version__)\n",
        "print(datasets.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(\"ratings_train.txt\", sep='\\t')\n",
        "test_df = pd.read_csv(\"ratings_test.txt\", sep='\\t')\n",
        "\n",
        "train_df.dropna(subset=['document'], inplace=True)\n",
        "test_df.dropna(subset=['document'], inplace=True)\n",
        "\n",
        "train_df.rename(columns={'label': 'labels'}, inplace=True)\n",
        "test_df.rename(columns={'label': 'labels'}, inplace=True)\n",
        "\n",
        "print(len(train_df),len(test_df))"
      ],
      "metadata": {},
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "149995 49997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
      ],
      "metadata": {},
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "    if \"attention\" in name and isinstance(module, torch.nn.Linear):\n",
        "        print(name, module)"
      ],
      "metadata": {},
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert.encoder.layer.0.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.0.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.0.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.0.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.1.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.1.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.1.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.1.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.2.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.2.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.2.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.2.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.3.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.3.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.3.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.3.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.4.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.4.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.4.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.4.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.5.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.5.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.5.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.5.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.6.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.6.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.6.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.6.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.7.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.7.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.7.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.7.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.8.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.8.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.8.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.8.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.9.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.9.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.9.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.9.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.10.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.10.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.10.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.10.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.11.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.11.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.11.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.11.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "        print(name, module)"
      ],
      "metadata": {},
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert.encoder.layer.0.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.0.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.0.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.0.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.0.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
            "bert.encoder.layer.0.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
            "bert.encoder.layer.1.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.1.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.1.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.1.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.1.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
            "bert.encoder.layer.1.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
            "bert.encoder.layer.2.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.2.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.2.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.2.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.2.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
            "bert.encoder.layer.2.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
            "bert.encoder.layer.3.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.3.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.3.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.3.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.3.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
            "bert.encoder.layer.3.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
            "bert.encoder.layer.4.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.4.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.4.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.4.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.4.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
            "bert.encoder.layer.4.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
            "bert.encoder.layer.5.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.5.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.5.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.5.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.5.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
            "bert.encoder.layer.5.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
            "bert.encoder.layer.6.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.6.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.6.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.6.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.6.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
            "bert.encoder.layer.6.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
            "bert.encoder.layer.7.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.7.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.7.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.7.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.7.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
            "bert.encoder.layer.7.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
            "bert.encoder.layer.8.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.8.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.8.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.8.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.8.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
            "bert.encoder.layer.8.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
            "bert.encoder.layer.9.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.9.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.9.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.9.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.9.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
            "bert.encoder.layer.9.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
            "bert.encoder.layer.10.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.10.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.10.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.10.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.10.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
            "bert.encoder.layer.10.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
            "bert.encoder.layer.11.attention.self.query Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.11.attention.self.key Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.11.attention.self.value Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.11.attention.output.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "bert.encoder.layer.11.intermediate.dense Linear(in_features=768, out_features=3072, bias=True)\n",
            "bert.encoder.layer.11.output.dense Linear(in_features=3072, out_features=768, bias=True)\n",
            "bert.pooler.dense Linear(in_features=768, out_features=768, bias=True)\n",
            "classifier Linear(in_features=768, out_features=2, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query\", \"value\"],  # BERT\uc5d0\uc11c\ub294 \uc77c\ubc18\uc801\uc73c\ub85c q, v\ub9cc \uc801\uc6a9\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLS\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {},
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 296,450 || all params: 110,915,332 || trainable%: 0.2673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {},
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 296,450 || all params: 110,915,332 || trainable%: 0.2673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "metadata": {},
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.2.attention.self.query.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.2.attention.self.query.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.2.attention.self.value.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.2.attention.self.value.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.3.attention.self.query.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.3.attention.self.query.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.3.attention.self.value.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.3.attention.self.value.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.4.attention.self.query.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.4.attention.self.query.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.4.attention.self.value.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.4.attention.self.value.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.5.attention.self.query.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.5.attention.self.query.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.5.attention.self.value.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.5.attention.self.value.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.6.attention.self.query.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.6.attention.self.query.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.6.attention.self.value.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.6.attention.self.value.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.7.attention.self.query.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.7.attention.self.query.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.7.attention.self.value.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.7.attention.self.value.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.8.attention.self.query.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.8.attention.self.query.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.8.attention.self.value.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.8.attention.self.value.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.9.attention.self.query.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.9.attention.self.query.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.9.attention.self.value.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.9.attention.self.value.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.10.attention.self.query.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.10.attention.self.query.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.10.attention.self.value.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.10.attention.self.value.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.11.attention.self.query.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.11.attention.self.query.lora_B.default.weight\n",
            "base_model.model.bert.encoder.layer.11.attention.self.value.lora_A.default.weight\n",
            "base_model.model.bert.encoder.layer.11.attention.self.value.lora_B.default.weight\n",
            "base_model.model.classifier.modules_to_save.default.weight\n",
            "base_model.model.classifier.modules_to_save.default.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = train_df[\"document\"].tolist()\n",
        "\n",
        "lengths = []\n",
        "\n",
        "for text in tqdm(texts):\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        truncation=False,\n",
        "    )\n",
        "    lengths.append(len(tokens[\"input_ids\"]))\n",
        "\n",
        "plt.hist(lengths, bins=30)\n",
        "plt.title(\"Tokenized Length Distribution (NSMC)\")\n",
        "plt.xlabel(\"Token Length\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"Max length:\", max(lengths))\n",
        "print(\"95th percentile:\", np.percentile(lengths, 95))\n",
        "print(\"Mean length:\", np.mean(lengths))\n",
        "print(\"Median length:\", np.median(lengths))"
      ],
      "metadata": {},
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/149995 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "819ecca8a6ee4f3a8c402ec75468d76f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length: 142\n",
            "95th percentile: 62.0\n",
            "Mean length: 22.276189206306878\n",
            "Median length: 17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = test_df[\"document\"].tolist()\n",
        "\n",
        "lengths = []\n",
        "\n",
        "for text in tqdm(texts):\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        truncation=False,\n",
        "    )\n",
        "    lengths.append(len(tokens[\"input_ids\"]))\n",
        "\n",
        "plt.hist(lengths, bins=30)\n",
        "plt.title(\"Tokenized Length Distribution (NSMC)\")\n",
        "plt.xlabel(\"Token Length\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"Max length:\", max(lengths))\n",
        "print(\"95th percentile:\", np.percentile(lengths, 95))\n",
        "print(\"Mean length:\", np.mean(lengths))\n",
        "print(\"Median length:\", np.median(lengths))"
      ],
      "metadata": {},
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/49997 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4075597dff8e47549653a0bd5b150ee9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length: 122\n",
            "95th percentile: 62.0\n",
            "Mean length: 22.360981658899533\n",
            "Median length: 17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.Dataset.from_pandas(train_df)"
      ],
      "metadata": {},
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {},
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'document', 'label', '__index_level_0__'],\n",
              "    num_rows: 149995\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.Dataset.from_pandas(train_df)\n",
        "valid_dataset = datasets.Dataset.from_pandas(test_df)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"document\"],\n",
        "        padding=\"max_length\",    # \ub610\ub294 True\ub85c \ud558\uba74 \ub3d9\uc801 padding\n",
        "        truncation=True,\n",
        "        max_length=192,\n",
        "        return_token_type_ids=True,\n",
        "    )\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "valid_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
      ],
      "metadata": {},
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/149995 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acac63cfcd314a688e41415d72eedbdb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/49997 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7c98296c0ba4647b66cd9d811da3dbe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers.utils import logging\n",
        "\n",
        "#logging.set_verbosity_info()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/results\",\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=256,\n",
        "    per_device_eval_batch_size=256,\n",
        "    gradient_accumulation_steps=1,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=250,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=250,\n",
        "    save_total_limit=1,\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",\n",
        "    greater_is_better=True,\n",
        "\n",
        "    num_train_epochs=8,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    bf16=True,  # \uc790\ub3d9\uc73c\ub85c accelerate\ub85c mixed precision \uc0ac\uc6a9\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    report_to=\"none\",\n",
        "    seed = 42,\n",
        "    warmup_steps = 500,\n",
        "    lr_scheduler_type=\"cosine\"\n",
        ")"
      ],
      "metadata": {},
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    acc = (preds == labels).mean()\n",
        "    return {\"accuracy\": acc}"
      ],
      "metadata": {},
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {},
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {},
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the Training set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 149,995\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 5,860\n",
            "  Number of trainable parameters = 296,450\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5860' max='5860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5860/5860 2:15:35, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.426600</td>\n",
              "      <td>0.348252</td>\n",
              "      <td>0.850931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.326400</td>\n",
              "      <td>0.310563</td>\n",
              "      <td>0.865172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.304000</td>\n",
              "      <td>0.291821</td>\n",
              "      <td>0.876893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.286800</td>\n",
              "      <td>0.283503</td>\n",
              "      <td>0.880933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.274500</td>\n",
              "      <td>0.273957</td>\n",
              "      <td>0.883973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.266700</td>\n",
              "      <td>0.270697</td>\n",
              "      <td>0.886033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.270700</td>\n",
              "      <td>0.263512</td>\n",
              "      <td>0.888973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.251100</td>\n",
              "      <td>0.260027</td>\n",
              "      <td>0.890353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.251000</td>\n",
              "      <td>0.259339</td>\n",
              "      <td>0.889913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.248600</td>\n",
              "      <td>0.258467</td>\n",
              "      <td>0.892554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.244000</td>\n",
              "      <td>0.255973</td>\n",
              "      <td>0.894034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.231600</td>\n",
              "      <td>0.253259</td>\n",
              "      <td>0.895714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>0.236300</td>\n",
              "      <td>0.253549</td>\n",
              "      <td>0.895574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.238200</td>\n",
              "      <td>0.252430</td>\n",
              "      <td>0.896254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>0.231100</td>\n",
              "      <td>0.259639</td>\n",
              "      <td>0.894614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.231100</td>\n",
              "      <td>0.253146</td>\n",
              "      <td>0.895454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4250</td>\n",
              "      <td>0.220600</td>\n",
              "      <td>0.254008</td>\n",
              "      <td>0.896554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.226500</td>\n",
              "      <td>0.250307</td>\n",
              "      <td>0.896474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4750</td>\n",
              "      <td>0.223600</td>\n",
              "      <td>0.250150</td>\n",
              "      <td>0.896674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.222600</td>\n",
              "      <td>0.251349</td>\n",
              "      <td>0.897234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5250</td>\n",
              "      <td>0.216300</td>\n",
              "      <td>0.250730</td>\n",
              "      <td>0.897454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.220500</td>\n",
              "      <td>0.250685</td>\n",
              "      <td>0.897094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5750</td>\n",
              "      <td>0.224300</td>\n",
              "      <td>0.250609</td>\n",
              "      <td>0.897214</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-250\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-500\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-250] due to args.save_total_limit\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-750\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-500] due to args.save_total_limit\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-1000\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-750] due to args.save_total_limit\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-1250\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-1000] due to args.save_total_limit\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-1500\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-1250] due to args.save_total_limit\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-1750\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-1500] due to args.save_total_limit\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-2000\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-1750] due to args.save_total_limit\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-2250\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-2500\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-2000] due to args.save_total_limit\n",
            "Deleting older checkpoint [/content/results/checkpoint-2250] due to args.save_total_limit\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-2750\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-2500] due to args.save_total_limit\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-3000\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-2750] due to args.save_total_limit\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-3250\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-3500\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-3000] due to args.save_total_limit\n",
            "Deleting older checkpoint [/content/results/checkpoint-3250] due to args.save_total_limit\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-3750\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-4000\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-3750] due to args.save_total_limit\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-4250\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-3500] due to args.save_total_limit\n",
            "Deleting older checkpoint [/content/results/checkpoint-4000] due to args.save_total_limit\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-4500\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-4750\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-4250] due to args.save_total_limit\n",
            "Deleting older checkpoint [/content/results/checkpoint-4500] due to args.save_total_limit\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-5000\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-4750] due to args.save_total_limit\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-5250\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-5000] due to args.save_total_limit\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-5500\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: document, __index_level_0__, id. If document, __index_level_0__, id are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 49997\n",
            "  Batch size = 256\n",
            "Saving model checkpoint to /content/results/checkpoint-5750\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-5500] due to args.save_total_limit\n",
            "Saving model checkpoint to /content/results/checkpoint-5860\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Deleting older checkpoint [/content/results/checkpoint-5750] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/results/checkpoint-5250 (score: 0.8974538472308339).\n",
            "Deleting older checkpoint [/content/results/checkpoint-5860] due to args.save_total_limit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5860, training_loss=0.2597612566508531, metrics={'train_runtime': 8137.5765, 'train_samples_per_second': 184.324, 'train_steps_per_second': 0.72, 'total_flos': 1.48507283832192e+17, 'train_loss': 0.2597612566508531, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"best_lora_model\")"
      ],
      "metadata": {},
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--klue--bert-base/snapshots/77c8b3d707df785034b4e50f2da5d37be5f0f546/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.52.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"best_lora_model\")"
      ],
      "metadata": {},
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizer config file saved in best_lora_model/tokenizer_config.json\n",
            "Special tokens file saved in best_lora_model/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('best_lora_model/tokenizer_config.json',\n",
              " 'best_lora_model/special_tokens_map.json',\n",
              " 'best_lora_model/vocab.txt',\n",
              " 'best_lora_model/added_tokens.json',\n",
              " 'best_lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# 1. PEFT config \uba3c\uc800 \ub85c\ub4dc\n",
        "peft_config = PeftConfig.from_pretrained(\"best_lora_model\")\n",
        "\n",
        "# 2. base \ubaa8\ub378 \uba3c\uc800 \ub85c\ub4dc\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(peft_config.base_model_name_or_path)\n",
        "\n",
        "# 3. LoRA weight \uc5b9\uae30\n",
        "model = PeftModel.from_pretrained(base_model, \"best_lora_model\")\n",
        "\n",
        "# 4. tokenizer\ub3c4 \uac19\uc774 \ub85c\ub4dc\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"best_lora_model\")"
      ],
      "metadata": {},
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"{name} is trainable\")"
      ],
      "metadata": {},
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.2.attention.self.query.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.2.attention.self.query.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.2.attention.self.value.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.2.attention.self.value.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.3.attention.self.query.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.3.attention.self.query.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.3.attention.self.value.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.3.attention.self.value.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.4.attention.self.query.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.4.attention.self.query.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.4.attention.self.value.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.4.attention.self.value.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.5.attention.self.query.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.5.attention.self.query.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.5.attention.self.value.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.5.attention.self.value.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.6.attention.self.query.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.6.attention.self.query.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.6.attention.self.value.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.6.attention.self.value.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.7.attention.self.query.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.7.attention.self.query.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.7.attention.self.value.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.7.attention.self.value.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.8.attention.self.query.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.8.attention.self.query.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.8.attention.self.value.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.8.attention.self.value.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.9.attention.self.query.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.9.attention.self.query.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.9.attention.self.value.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.9.attention.self.value.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.10.attention.self.query.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.10.attention.self.query.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.10.attention.self.value.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.10.attention.self.value.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.11.attention.self.query.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.11.attention.self.query.lora_B.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.11.attention.self.value.lora_A.default.weight is trainable\n",
            "base_model.model.bert.encoder.layer.11.attention.self.value.lora_B.default.weight is trainable\n",
            "base_model.model.classifier.modules_to_save.default.weight is trainable\n",
            "base_model.model.classifier.modules_to_save.default.bias is trainable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.enable_adapter_layers()"
      ],
      "metadata": {},
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {},
      "execution_count": 10,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='221' max='4688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 221/4688 03:55 < 1:20:11, 0.93 it/s, Epoch 0.38/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='756' max='4688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 756/4688 17:29 < 1:31:11, 0.72 it/s, Epoch 1.29/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.224900</td>\n",
              "      <td>0.251604</td>\n",
              "      <td>0.898314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.224600</td>\n",
              "      <td>0.256595</td>\n",
              "      <td>0.896034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.223700</td>\n",
              "      <td>0.256101</td>\n",
              "      <td>0.896654</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2559\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m                     ):\n\u001b[1;32m   2562\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft.utils import get_peft_model_state_dict\n",
        "\n",
        "torch.save(get_peft_model_state_dict(model), \"lora_only.pt\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.5r=16, cosine restart lr"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query\", \"value\"],  # BERT\uc5d0\uc11c\ub294 \uc77c\ubc18\uc801\uc73c\ub85c q, v\ub9cc \uc801\uc6a9\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLS\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {},
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {},
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 591,362 || all params: 111,210,244 || trainable%: 0.5318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "\n",
        "class CosineRestartCallback(TrainerCallback):\n",
        "    def __init__(self, T_0, T_mult=1, eta_min=1e-6, warmup_steps=500, base_lr=3e-4):\n",
        "        self.T_0 = T_0\n",
        "        self.T_mult = T_mult\n",
        "        self.eta_min = eta_min\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.scheduler = None\n",
        "        self.optimizer = None\n",
        "        self.base_lr = base_lr\n",
        "\n",
        "    def on_train_begin(self, args, state, control, model=None, **kwargs):\n",
        "        self.optimizer = kwargs[\"optimizer\"]\n",
        "\n",
        "        self.scheduler = CosineAnnealingWarmRestarts(\n",
        "            self.optimizer,\n",
        "            T_0=self.T_0,\n",
        "            T_mult=self.T_mult,\n",
        "            eta_min=self.eta_min,\n",
        "        )\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step >= self.warmup_steps:\n",
        "            self.scheduler.step()\n",
        "        else :\n",
        "          lr = self.base_lr * state.global_step / self.warmup_steps\n",
        "          for param_group in self.optimizer.param_groups:\n",
        "              param_group['lr'] = lr\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs is not None and self.scheduler is not None:\n",
        "            if state.global_step >= self.warmup_steps:\n",
        "                current_lr = self.scheduler.get_last_lr()[0]\n",
        "            else:\n",
        "                current_lr = self.base_lr * state.global_step / self.warmup_steps\n",
        "            logs[\"custom_learning_rate\"] = current_lr"
      ],
      "metadata": {},
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/results\",\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=256,\n",
        "    per_device_eval_batch_size=256,\n",
        "    gradient_accumulation_steps=1,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=250,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=250,\n",
        "    save_total_limit=1,\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",\n",
        "    greater_is_better=True,\n",
        "\n",
        "    num_train_epochs=10,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    bf16=True,  # \uc790\ub3d9\uc73c\ub85c accelerate\ub85c mixed precision \uc0ac\uc6a9\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    report_to=\"tensorboard\",\n",
        "    seed = 42,\n",
        "    #warmup_steps = 500,\n",
        ")"
      ],
      "metadata": {},
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "warmup = 500\n",
        "total_step = math.ceil((len(train_df)/256) * 10) - warmup\n",
        "t_mult = 1\n",
        "n_cycle = 3\n",
        "t0 = math.ceil(total_step / n_cycle)\n",
        "print(t0)"
      ],
      "metadata": {},
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[CosineRestartCallback(T_0=t0, T_mult=1, eta_min=1e-6)],\n",
        ")"
      ],
      "metadata": {},
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {},
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5860' max='5860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5860/5860 2:15:54, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Learning Rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.402800</td>\n",
              "      <td>0.331812</td>\n",
              "      <td>0.855711</td>\n",
              "      <td>0.000150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.317400</td>\n",
              "      <td>0.304927</td>\n",
              "      <td>0.867352</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.297300</td>\n",
              "      <td>0.285213</td>\n",
              "      <td>0.879573</td>\n",
              "      <td>0.000286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.278400</td>\n",
              "      <td>0.278515</td>\n",
              "      <td>0.883013</td>\n",
              "      <td>0.000246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.264600</td>\n",
              "      <td>0.269078</td>\n",
              "      <td>0.886213</td>\n",
              "      <td>0.000188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.254300</td>\n",
              "      <td>0.269408</td>\n",
              "      <td>0.886673</td>\n",
              "      <td>0.000122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.259800</td>\n",
              "      <td>0.258776</td>\n",
              "      <td>0.891233</td>\n",
              "      <td>0.000063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.242200</td>\n",
              "      <td>0.260246</td>\n",
              "      <td>0.892094</td>\n",
              "      <td>0.000020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.242700</td>\n",
              "      <td>0.258200</td>\n",
              "      <td>0.892694</td>\n",
              "      <td>0.000001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.255400</td>\n",
              "      <td>0.259789</td>\n",
              "      <td>0.891573</td>\n",
              "      <td>0.000290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.249300</td>\n",
              "      <td>0.256438</td>\n",
              "      <td>0.893974</td>\n",
              "      <td>0.000253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.229100</td>\n",
              "      <td>0.254084</td>\n",
              "      <td>0.895554</td>\n",
              "      <td>0.000197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>0.234100</td>\n",
              "      <td>0.250858</td>\n",
              "      <td>0.896754</td>\n",
              "      <td>0.000132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.232900</td>\n",
              "      <td>0.249412</td>\n",
              "      <td>0.898074</td>\n",
              "      <td>0.000071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>0.221900</td>\n",
              "      <td>0.251633</td>\n",
              "      <td>0.897234</td>\n",
              "      <td>0.000024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.221600</td>\n",
              "      <td>0.249182</td>\n",
              "      <td>0.897934</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4250</td>\n",
              "      <td>0.222700</td>\n",
              "      <td>0.255500</td>\n",
              "      <td>0.897814</td>\n",
              "      <td>0.000293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.228700</td>\n",
              "      <td>0.256656</td>\n",
              "      <td>0.895994</td>\n",
              "      <td>0.000260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4750</td>\n",
              "      <td>0.224100</td>\n",
              "      <td>0.254236</td>\n",
              "      <td>0.898094</td>\n",
              "      <td>0.000206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.214100</td>\n",
              "      <td>0.248111</td>\n",
              "      <td>0.898814</td>\n",
              "      <td>0.000142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5250</td>\n",
              "      <td>0.207200</td>\n",
              "      <td>0.247615</td>\n",
              "      <td>0.900074</td>\n",
              "      <td>0.000079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.200800</td>\n",
              "      <td>0.250138</td>\n",
              "      <td>0.901014</td>\n",
              "      <td>0.000030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5750</td>\n",
              "      <td>0.204400</td>\n",
              "      <td>0.248837</td>\n",
              "      <td>0.900974</td>\n",
              "      <td>0.000004</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5860, training_loss=0.25231625081736075, metrics={'train_runtime': 8155.6848, 'train_samples_per_second': 183.915, 'train_steps_per_second': 0.719, 'total_flos': 1.490168747812608e+17, 'train_loss': 0.25231625081736075, 'epoch': 10.0, 'custom_learning_rate': 0.0003})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {},
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3146' max='5860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3146/5860 1:12:26 < 1:02:32, 0.72 it/s, Epoch 5.37/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Learning Rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.205800</td>\n",
              "      <td>0.251076</td>\n",
              "      <td>0.900834</td>\n",
              "      <td>0.000150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.210600</td>\n",
              "      <td>0.253411</td>\n",
              "      <td>0.899614</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.206200</td>\n",
              "      <td>0.251301</td>\n",
              "      <td>0.900054</td>\n",
              "      <td>0.000286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.209100</td>\n",
              "      <td>0.255641</td>\n",
              "      <td>0.897134</td>\n",
              "      <td>0.000246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.197500</td>\n",
              "      <td>0.249604</td>\n",
              "      <td>0.899894</td>\n",
              "      <td>0.000188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.187700</td>\n",
              "      <td>0.250786</td>\n",
              "      <td>0.900714</td>\n",
              "      <td>0.000122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.198600</td>\n",
              "      <td>0.249343</td>\n",
              "      <td>0.902234</td>\n",
              "      <td>0.000063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.180100</td>\n",
              "      <td>0.256110</td>\n",
              "      <td>0.902094</td>\n",
              "      <td>0.000020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.182200</td>\n",
              "      <td>0.252635</td>\n",
              "      <td>0.901674</td>\n",
              "      <td>0.000001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.192700</td>\n",
              "      <td>0.252306</td>\n",
              "      <td>0.899734</td>\n",
              "      <td>0.000290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.191800</td>\n",
              "      <td>0.254007</td>\n",
              "      <td>0.901034</td>\n",
              "      <td>0.000253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.173800</td>\n",
              "      <td>0.257121</td>\n",
              "      <td>0.901474</td>\n",
              "      <td>0.000197</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2559\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m                     ):\n\u001b[1;32m   2562\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.bucketing"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.Dataset.from_pandas(train_df)\n",
        "valid_dataset = datasets.Dataset.from_pandas(test_df)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"document\"],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_token_type_ids=True,\n",
        "    )\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "valid_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
      ],
      "metadata": {},
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/149995 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "891fc9a47ece4126b675745005388e5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/49997 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c3ea084f2ab48cb83ec35724c66bc08"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {},
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/results2\",\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=256,\n",
        "    per_device_eval_batch_size=256,\n",
        "    gradient_accumulation_steps=1,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=250,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=250,\n",
        "    save_total_limit=1,\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",\n",
        "    greater_is_better=True,\n",
        "\n",
        "    num_train_epochs=10,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_dir=\"./logs2\",\n",
        "    logging_steps=100,\n",
        "    group_by_length= True,\n",
        "    bf16=True,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    report_to=\"tensorboard\",\n",
        "    seed = 42,\n",
        ")"
      ],
      "metadata": {},
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {},
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {},
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5860' max='5860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5860/5860 1:17:48, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.326900</td>\n",
              "      <td>0.308715</td>\n",
              "      <td>0.867932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.297400</td>\n",
              "      <td>0.284254</td>\n",
              "      <td>0.878453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.279300</td>\n",
              "      <td>0.277312</td>\n",
              "      <td>0.882113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.263300</td>\n",
              "      <td>0.270878</td>\n",
              "      <td>0.886553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.265200</td>\n",
              "      <td>0.263619</td>\n",
              "      <td>0.889433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.255600</td>\n",
              "      <td>0.259833</td>\n",
              "      <td>0.892174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.251300</td>\n",
              "      <td>0.263009</td>\n",
              "      <td>0.892254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.238600</td>\n",
              "      <td>0.258870</td>\n",
              "      <td>0.893614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.241300</td>\n",
              "      <td>0.252913</td>\n",
              "      <td>0.896794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.223100</td>\n",
              "      <td>0.255185</td>\n",
              "      <td>0.895714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.230800</td>\n",
              "      <td>0.253311</td>\n",
              "      <td>0.896954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.225500</td>\n",
              "      <td>0.252556</td>\n",
              "      <td>0.897914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>0.225300</td>\n",
              "      <td>0.252178</td>\n",
              "      <td>0.899434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.220600</td>\n",
              "      <td>0.248958</td>\n",
              "      <td>0.899054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>0.214100</td>\n",
              "      <td>0.250711</td>\n",
              "      <td>0.899754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.211300</td>\n",
              "      <td>0.253895</td>\n",
              "      <td>0.899474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4250</td>\n",
              "      <td>0.202500</td>\n",
              "      <td>0.248795</td>\n",
              "      <td>0.900614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.211100</td>\n",
              "      <td>0.248047</td>\n",
              "      <td>0.899474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4750</td>\n",
              "      <td>0.207900</td>\n",
              "      <td>0.253118</td>\n",
              "      <td>0.900074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.197900</td>\n",
              "      <td>0.253873</td>\n",
              "      <td>0.900434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5250</td>\n",
              "      <td>0.203400</td>\n",
              "      <td>0.248622</td>\n",
              "      <td>0.901154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.197600</td>\n",
              "      <td>0.250146</td>\n",
              "      <td>0.900634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5750</td>\n",
              "      <td>0.197200</td>\n",
              "      <td>0.250491</td>\n",
              "      <td>0.901434</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5860, training_loss=0.23546576955619525, metrics={'train_runtime': 4670.1914, 'train_samples_per_second': 321.175, 'train_steps_per_second': 1.255, 'total_flos': 7.972106257680262e+16, 'train_loss': 0.23546576955619525, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./runs"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.\ud68c\uace0"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "LoRA r=8, \ud559\uc2b5 4/5 \uc9c0\uc810\uc5d0\uc11c \uacfc\uc801\ud569\uc774 \ubc1c\uc0dd\ud55c \ud6c4 train loss\ub610\ud55c \uac1c\uc120\ub418\uc9c0 \uc54a\uace0 \uc9c4\ub3d9\ud568.\n",
        "cosine decay lr\uc758 \uc601\ud5a5\uc77c \uc218 \uc788\uc5b4\uc11c warmup step\ubd80\ud130 \ub2e4\uc2dc \ud559\uc2b5\ud558\uc600\uc9c0\ub9cc train loss\ub294 \uc5ec\uc804\ud788 \uac10\uc18c\ud558\uc9c0 \uc54a\uc74c\n",
        "\ubaa8\ub378\uc758 \ud45c\ud604\ub825\uc774 \ubd80\uc871\ud574\uc11c \ubc1c\uc0dd\ud55c \uacfc\uc801\ud569\uc774\ub77c\uace0 \ud310\ub2e8\ub428."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "r=16 \uc5d0\uc11c train loss\uc640 valid loss\uc758 \ucc28\uc774\uac00 \ub354 \ucee4\uc84c\uace0 valid accuracy\ub3c4 \uac1c\uc120\ub428."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "bucketing \uc5d0 \uae30\ub300\ub418\ub294 \ud6a8\uacfc\ub294 \ub2e4\uc74c\uacfc \uac19\uc74c.\n",
        "1. \ub3d9\uc801 padding\uc73c\ub85c \uc5f0\uc0b0\ub7c9 \ubc0f \uba54\ubaa8\ub9ac \ud6a8\uc728 -> \ud559\uc2b5 \ub9ac\uc18c\uc2a4\uc640 \ud559\uc2b5\uc2dc\uac04\n",
        "2. loss, gradient variance\uac00 \uac10\uc18c\ud558\uc5ec learning signal\uc774 \ub354 \ud6a8\uacfc\uc801\uc73c\ub85c \ubc1c\uc0dd, \ud3c9\uade0\uacfc\uc815\uc5d0\uc11c \ud76c\uc11d\uc774 \ubc1c\uc0dd\ud558\uc9c0 \uc54a\uc74c\n",
        "3. \uae38\uc774\uc640 label\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \uc788\ub294 \uacbd\uc6b0 bucketing\uc740 \uadf8 \ud3b8\ud5a5\uc744 \ub354 \ubaa8\ub378\uc5d0 \uac15\ud558\uac8c \uc8fc\uc785\ud560 \uc6b0\ub824\uac00 \uc788\uc74c\n",
        "4. \uae30\ubcf8\uc801\uc73c\ub85c grad update\ub294 \uc21c\ucc28\uc801\uc73c\ub85c \uc774\ub8e8\uc5b4\uc9c0\ubbc0\ub85c \uc55e\uc758 update\uc5d0 \uc601\ud5a5\uc744 \ubc1b\ub294\ub370 grad \ubc29\ud5a5\uc774 \uc815\uc81c\ub418\uba74\uc11c \ud55c\ucabd \ubc29\ud5a5\uc73c\ub85c \uc5c5\ub370\uc774\ud2b8\uac00 \ucee4\uc9c0\ubbc0\ub85c \ud0d0\uc0c9\uc774 \uc904\uc5b4\ub4e4\uace0 sharp minima\uc5d0 \ube60\uc9c8 \uc704\ud5d8\uc740 \ub354 \ud07c\n",
        "5. SGD\uc758 \uae30\ubcf8\uc6d0\ub9ac\ub294 mini batch\uac00 global batch\uc640 \ud1b5\uacc4\uc801\uc73c\ub85c \uc720\uc0ac \ub610\ub294 \ub300\ud45c\ud55c\ub2e4\ub294 \uac83\uc778\ub370 bucketing\uc740 \uc774 \uc6d0\ub9ac\ub97c \ucca0\uc800\ud788 \ubb34\uc2dc\ud568\n",
        "6."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\ud559\uc2b5 \uc2dc\uac04\uc774\ub098 \uba54\ubaa8\ub9ac (16 -> 12.5gb), 1\uc2dc\uac0440\ubd84 -> 1\uc2dc\uac04\uc73c\ub85c \uac10\uc18c\ud558\uc600\uc74c. max_length\ub97c \uc77c\ubd80\ub7ec \ub109\ub109\ud788 \uc92c\ub294\ub370 \uadf8\uac83\uc744 \ud0c0\uc774\ud2b8\ud558\uac8c \ub9de\ucd94\uba74 \ucc28\uc774\ub294 \uc904\uc5b4\ub4e4 \uac83\uc73c\ub85c \ubcf4\uc784."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\uc774 task\uc5d0 \uc9d1\uc911\ud558\uc790\uba74 \uc77c\ub2e8 \uc9e7\uc740 \uae38\uc774 \ub0b4\uc5d0\uc11c 1\ube60\ub2e4 \uc774\ub7f0 \ub178\uc774\uc988\ub098 good\uc778\ub370 \ub77c\ubca8\uc774 \ub2e4\ub978 \ub178\uc774\uc988\uac00 \ub9ce\uc740\ub370 \uadf8\ub7f0 \uac83\ub4e4\uc744 \ud55c \ubc30\uce58\uc5d0 \ubaa8\uc544\ub193\uc73c\uba74 grad \ubc29\ud5a5\uc774 \uc0c1\ucda9\ub418\uc11c \uac70\uc758 \ub098\uc624\uc9c0 \uc54a\uac70\ub098 \ud76c\uc11d\ub418\uc11c \uc804\uccb4 \ud559\uc2b5\uc5d0 \ubd80\uc815\uc801\uc778 \uc601\ud5a5\uc774 \ub35c\ud560 \uac83 \uac19\uc74c. \uc989, \uc4f0\ub808\uae30\ub97c \ubaa8\uc544\uc11c \ubc84\ub9ac\ub294 \uac70\uac19\uc740 \ub290\ub08c? -> \uadf8\ub0e5 \uae38\uc774 \uc9e7\uc740 \uc0d8\ud50c \ubc84\ub9ac\uace0 \ud559\uc2b5\uc2dc\ucf1c\ub3c4 \ub418\ub294 \ubb38\uc81c\n",
        "\n",
        "batchsize\ub97c 256\uc73c\ub85c \uc918\uc11c bucketing\uc774 \uac00\uc838\uc62c \uc218 \uc788\ub294 \ub2e4\uc591\uc131 \ubd80\uc871 \ubb38\uc81c\uac00 \uc5b4\ub290\uc815\ub3c4 \uc644\ud654\ub418\uc5c8\uc744 \uac00\ub2a5\uc131\n",
        "\n",
        "task \uc790\uccb4\uac00 \ucc3e\uc544\uc57c\ud558\ub294 minima\uac00 task\uac00 \uc774\uc9c4\ubd84\ub958\uc774\uace0 \uc26c\uc6cc\uc11c sharp minima\uc5d0 \ube60\uc9c8 \uc704\ud5d8\uc790\uccb4\uac00 \uc801\uc740 \uac83\uc740 \uc544\ub2cc\uc9c0?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch suffle\uc740 \uc65c\ud558\ub294\uac00\n",
        "\uc9e7\uc740 \uae38\uc774\uc5d0 noise\uac00 \ub9ce\uc740\ub370 noise\uac00 \ud55c batch \uc5d0 \ub4e4\uc5b4\uac00\uba74 \uc5b4\ub5bb\uac8c \ub418\ub294\n"
      ],
      "metadata": {}
    }
  ]
}